{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eye tracking has been gaining in popularity over the past decade as a window into observers’ visual and cognitive processes. This project uses eye movement data from dyslexic kids to predict the possibility of dyslexia based on measured eye movements. The source for this data is available at https://doi.org/10.6084/m9.figshare.c.3521379.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from matplotlib import mlab\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "from scipy.stats import norm\n",
    "labels = [1, 1, 0, 0] #1: Disabled\n",
    "gender = [1,0,1,0] #1: Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawConfusionMatrix(cnf_matrix):\n",
    "    print(cnf_matrix)\n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the processed file with the features - the file contains one row per subject, 185 subjects and 52 features per subject. The data quality is pretty high. In the previous modules the data has been tested for the data to not cotain any missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape =  (185, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DistanceL</th>\n",
       "      <th>DistanceR</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Label</th>\n",
       "      <th>Subject</th>\n",
       "      <th>LTypeFSum</th>\n",
       "      <th>LTypeSSum</th>\n",
       "      <th>LTypeFCount</th>\n",
       "      <th>LTypeSCount</th>\n",
       "      <th>LTypeFMean</th>\n",
       "      <th>...</th>\n",
       "      <th>LVerDirNMean</th>\n",
       "      <th>RVerDirUSum</th>\n",
       "      <th>RVerDirDSum</th>\n",
       "      <th>RVerDirNSum</th>\n",
       "      <th>RVerDirUCount</th>\n",
       "      <th>RVerDirDCount</th>\n",
       "      <th>RVerDirNCount</th>\n",
       "      <th>RVerDirUMean</th>\n",
       "      <th>RVerDirDMean</th>\n",
       "      <th>RVerDirNMean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13413.296257</td>\n",
       "      <td>13580.306540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111GM3</td>\n",
       "      <td>15320.0</td>\n",
       "      <td>14520.0</td>\n",
       "      <td>171</td>\n",
       "      <td>172</td>\n",
       "      <td>89.590643</td>\n",
       "      <td>...</td>\n",
       "      <td>62.027027</td>\n",
       "      <td>11220.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>8860.0</td>\n",
       "      <td>262</td>\n",
       "      <td>235</td>\n",
       "      <td>148</td>\n",
       "      <td>42.824427</td>\n",
       "      <td>42.127660</td>\n",
       "      <td>60.272109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8788.682167</td>\n",
       "      <td>8509.062191</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111JA2</td>\n",
       "      <td>22100.0</td>\n",
       "      <td>17840.0</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>77.003484</td>\n",
       "      <td>...</td>\n",
       "      <td>43.737024</td>\n",
       "      <td>13740.0</td>\n",
       "      <td>13520.0</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>324</td>\n",
       "      <td>317</td>\n",
       "      <td>302</td>\n",
       "      <td>42.407407</td>\n",
       "      <td>42.784810</td>\n",
       "      <td>41.986755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9765.357380</td>\n",
       "      <td>10281.893102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111RP1</td>\n",
       "      <td>24780.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>239</td>\n",
       "      <td>240</td>\n",
       "      <td>103.682008</td>\n",
       "      <td>...</td>\n",
       "      <td>47.918367</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>13300.0</td>\n",
       "      <td>11860.0</td>\n",
       "      <td>326</td>\n",
       "      <td>357</td>\n",
       "      <td>260</td>\n",
       "      <td>45.030675</td>\n",
       "      <td>37.254902</td>\n",
       "      <td>45.791506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11950.957324</td>\n",
       "      <td>11461.339153</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112JU3</td>\n",
       "      <td>23920.0</td>\n",
       "      <td>15960.0</td>\n",
       "      <td>243</td>\n",
       "      <td>244</td>\n",
       "      <td>98.436214</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>10600.0</td>\n",
       "      <td>13300.0</td>\n",
       "      <td>15980.0</td>\n",
       "      <td>270</td>\n",
       "      <td>360</td>\n",
       "      <td>329</td>\n",
       "      <td>39.259259</td>\n",
       "      <td>36.944444</td>\n",
       "      <td>48.719512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4959.743932</td>\n",
       "      <td>4913.022136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>112KA1</td>\n",
       "      <td>29920.0</td>\n",
       "      <td>9760.0</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>120.645161</td>\n",
       "      <td>...</td>\n",
       "      <td>33.717579</td>\n",
       "      <td>12420.0</td>\n",
       "      <td>15920.0</td>\n",
       "      <td>11580.0</td>\n",
       "      <td>400</td>\n",
       "      <td>457</td>\n",
       "      <td>355</td>\n",
       "      <td>31.050000</td>\n",
       "      <td>34.912281</td>\n",
       "      <td>32.619718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DistanceL     DistanceR  Gender  Label Subject  LTypeFSum  LTypeSSum  \\\n",
       "0  13413.296257  13580.306540       1      0  111GM3    15320.0    14520.0   \n",
       "1   8788.682167   8509.062191       0      1  111JA2    22100.0    17840.0   \n",
       "2   9765.357380  10281.893102       1      1  111RP1    24780.0    15180.0   \n",
       "3  11950.957324  11461.339153       1      0  112JU3    23920.0    15960.0   \n",
       "4   4959.743932   4913.022136       1      1  112KA1    29920.0     9760.0   \n",
       "\n",
       "   LTypeFCount  LTypeSCount  LTypeFMean  ...  LVerDirNMean  RVerDirUSum  \\\n",
       "0          171          172   89.590643  ...     62.027027      11220.0   \n",
       "1          288          288   77.003484  ...     43.737024      13740.0   \n",
       "2          239          240  103.682008  ...     47.918367      14680.0   \n",
       "3          243          244   98.436214  ...     46.666667      10600.0   \n",
       "4          249          249  120.645161  ...     33.717579      12420.0   \n",
       "\n",
       "   RVerDirDSum  RVerDirNSum  RVerDirUCount  RVerDirDCount  RVerDirNCount  \\\n",
       "0       9900.0       8860.0            262            235            148   \n",
       "1      13520.0      12680.0            324            317            302   \n",
       "2      13300.0      11860.0            326            357            260   \n",
       "3      13300.0      15980.0            270            360            329   \n",
       "4      15920.0      11580.0            400            457            355   \n",
       "\n",
       "   RVerDirUMean  RVerDirDMean  RVerDirNMean  \n",
       "0     42.824427     42.127660     60.272109  \n",
       "1     42.407407     42.784810     41.986755  \n",
       "2     45.030675     37.254902     45.791506  \n",
       "3     39.259259     36.944444     48.719512  \n",
       "4     31.050000     34.912281     32.619718  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\Springboard\\Springboard01\\Capstone\\Capstone 2\\Data\\datawithfeatures.csv')\n",
    "print(\"Input Data Shape = \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependent Variable**\n",
    "\n",
    "The column **label** is the prediction column. This is the dependent variable. 0 is control group and 1 is dyslexic group. \n",
    "\n",
    "**Independent Variables**\n",
    "\n",
    "There are 52 independent variables.All but gender are numerical/decimal variables. The **gender** column is a categorical variable and  has 1 for males and 0 for females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaledData = print(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    88\n",
       "1    97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by =['Label']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 88 subjects in the control group and 97 subjects in the dyslexic group.  This way the data seems to be very balanced. use StandardScaler to scale the variables to a standard normal format.\n",
    "\n",
    "A training and validation approach is used. the available data is split with 70% of the data used for training the model and 30% of the data used to validate the model as the test data set. The dependent and independent variables are split into X_train and y_train for the training data set and X_test and y_test for the testing dataset. A stratified ratio preserved rows are split across the test and training data. The original data has a conrol/dyslexis ratio of (88/97) 90%, the training data has a ratio of (61/68) 90% and the test data has a ratio of (27/29) 93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Label\n",
      "0.0    61\n",
      "1.0    68\n",
      "dtype: int64\n",
      "Test Data Label\n",
      "0.0    27\n",
      "1.0    29\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['Label', 'Subject'], axis=1)\n",
    "X[:]= scaler.fit_transform(X[:])\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=123, shuffle=True)\n",
    "X_train = X_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_test = y_test.astype('float64')\n",
    "print(\"Training Data\", pd.concat([X_train, y_train], axis=1).groupby(by =['Label']).size())\n",
    "print (\"Test Data\",pd.concat([X_test, y_test], axis=1).groupby(by =['Label']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the hyperparameters for a logistic regression model, we try with different C values, 1000 through 0.001. Looking at the results a C value of 10 provides the best training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------  using liblinear and L1 penalty ------------------------\n",
      "C: 1000\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8571428571428571\n",
      "\n",
      "C: 100\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.875\n",
      "\n",
      "C: 10\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.9285714285714286\n",
      "\n",
      "C: 5\n",
      "Training accuracy: 0.9844961240310077\n",
      "Test accuracy: 0.9107142857142857\n",
      "\n",
      "C: 1\n",
      "Training accuracy: 0.9689922480620154\n",
      "Test accuracy: 0.875\n",
      "\n",
      "C: 0.1\n",
      "Training accuracy: 0.9224806201550387\n",
      "Test accuracy: 0.875\n",
      "\n",
      "C: 0.001\n",
      "Training accuracy: 0.4728682170542636\n",
      "Test accuracy: 0.48214285714285715\n",
      "\n",
      "-----------------  using liblinear and L2 penalty ------------------------\n",
      "C: 1000\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8571428571428571\n",
      "\n",
      "C: 100\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8928571428571429\n",
      "\n",
      "C: 10\n",
      "Training accuracy: 0.9844961240310077\n",
      "Test accuracy: 0.8928571428571429\n",
      "\n",
      "C: 5\n",
      "Training accuracy: 0.9844961240310077\n",
      "Test accuracy: 0.8928571428571429\n",
      "\n",
      "C: 1\n",
      "Training accuracy: 0.9689922480620154\n",
      "Test accuracy: 0.9107142857142857\n",
      "\n",
      "C: 0.1\n",
      "Training accuracy: 0.9457364341085271\n",
      "Test accuracy: 0.9464285714285714\n",
      "\n",
      "C: 0.001\n",
      "Training accuracy: 0.937984496124031\n",
      "Test accuracy: 0.8571428571428571\n",
      "\n",
      "-----------------  using lbfgs ------------------------\n",
      "C: 1000\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8571428571428571\n",
      "\n",
      "C: 100\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8928571428571429\n",
      "\n",
      "C: 10\n",
      "Training accuracy: 0.9844961240310077\n",
      "Test accuracy: 0.8928571428571429\n",
      "\n",
      "C: 5\n",
      "Training accuracy: 0.9844961240310077\n",
      "Test accuracy: 0.875\n",
      "\n",
      "C: 1\n",
      "Training accuracy: 0.9689922480620154\n",
      "Test accuracy: 0.9107142857142857\n",
      "\n",
      "C: 0.1\n",
      "Training accuracy: 0.937984496124031\n",
      "Test accuracy: 0.9285714285714286\n",
      "\n",
      "C: 0.001\n",
      "Training accuracy: 0.9457364341085271\n",
      "Test accuracy: 0.875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [1000, 100, 10, 5, 1, .1, .001]\n",
    "    \n",
    "print (\"-----------------  using liblinear and L1 penalty ------------------------\")\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "    print('Training accuracy:', clf.score(X_train, y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    print('')\n",
    "    C = [1000, 100, 10, 5, 1, .1, .001]\n",
    "    \n",
    "print( \"-----------------  using liblinear and L2 penalty ------------------------\")\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l2', C=c, solver='liblinear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "    print('Training accuracy:', clf.score(X_train, y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    print('')\n",
    "    C = [1000, 100, 10, 5, 1, .1, .001]\n",
    "    \n",
    "print (\"-----------------  using lbfgs ------------------------\")\n",
    "\n",
    "for c in C:\n",
    "    clf = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "    print('Training accuracy:', clf.score(X_train, y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to fit a Logistic regression model with a regularization parameter (C) of 10 and a maximum of 500 iterations. Then test the model using predictions from the test data set.  We prepare and print the classification report for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        61\n",
      "         1.0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.89      0.92        27\n",
      "         1.0       0.90      0.97      0.93        29\n",
      "\n",
      "    accuracy                           0.93        56\n",
      "   macro avg       0.93      0.93      0.93        56\n",
      "weighted avg       0.93      0.93      0.93        56\n",
      "\n",
      "[[24  3]\n",
      " [ 1 28]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=500, C=10).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_predtrain = model.predict(X_train)\n",
    "print(classification_report(y_train, y_predtrain)) \n",
    "print(classification_report(y_test, y_pred))\n",
    "drawConfusionMatrix(confusion_matrix(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying with the Gradient Boosting Classifier, to see if there are any better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        61\n",
      "         1.0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79        27\n",
      "         1.0       0.80      0.83      0.81        29\n",
      "\n",
      "    accuracy                           0.80        56\n",
      "   macro avg       0.80      0.80      0.80        56\n",
      "weighted avg       0.80      0.80      0.80        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBMmodel = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = GBMmodel.predict(X_test)\n",
    "y_predtrain = GBMmodel.predict(X_train)\n",
    "print(classification_report(y_train, y_predtrain)) \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the linear regression with solver = liblinear and L1 penalty provides the best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
